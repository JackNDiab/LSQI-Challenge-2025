{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c436c7d4-f35e-443d-af23-ad4971ba8be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found shards: 50 first/last: ./dataset/mpirank_0 ./dataset/mpirank_49\n",
      "Total samples: 10000000\n"
     ]
    }
   ],
   "source": [
    "import os, glob, json, time, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "import ClassicalPredictor as CP\n",
    "import TrajectoryGenerator as TG\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Sharded memmap utilities\n",
    "# --------------------------\n",
    "import os, glob\n",
    "import re\n",
    "\n",
    "def discover_mpirank_shards(base_dir: str):\n",
    "    shard_dirs = sorted(glob.glob(os.path.join(base_dir, \"mpirank_*\")))\n",
    "    shard_dirs = [d for d in shard_dirs if os.path.isdir(d)]\n",
    "    if not shard_dirs:\n",
    "        raise FileNotFoundError(f\"No mpirank_* dirs found under: {base_dir}\")\n",
    "\n",
    "    def rank_of(path):\n",
    "        m = re.search(r\"mpirank_(\\d+)$\", path)\n",
    "        return int(m.group(1)) if m else 10**9\n",
    "\n",
    "    shard_dirs.sort(key=rank_of)\n",
    "    return shard_dirs\n",
    "\n",
    "def shard_n_written(shard_dir: str) -> int:\n",
    "    meta_path = os.path.join(shard_dir, \"meta.npz\")\n",
    "    if not os.path.exists(meta_path):\n",
    "        raise FileNotFoundError(f\"Missing meta.npz in {shard_dir}\")\n",
    "    meta = np.load(meta_path)\n",
    "    return int(meta[\"n_written\"])\n",
    "\n",
    "class ShardedMemmap:\n",
    "    \"\"\"\n",
    "    Virtual concatenation over shards for a single .npy array.\n",
    "    Uses numpy memmap to avoid loading into RAM.\n",
    "    \"\"\"\n",
    "    def __init__(self, shard_dirs, filename: str, dtype=None):\n",
    "        self.arrs = []\n",
    "        self.cum = [0]\n",
    "        self.shard_dirs = shard_dirs\n",
    "\n",
    "        for sd in shard_dirs:\n",
    "            n = shard_n_written(sd)\n",
    "            path = os.path.join(sd, filename)\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"Missing {path}\")\n",
    "            arr = np.load(path, mmap_mode=\"r\")\n",
    "            # keep only valid rows\n",
    "            if arr.shape[0] < n:\n",
    "                raise ValueError(f\"{path} has {arr.shape[0]} rows but meta says n_written={n}\")\n",
    "            arr = arr[:n]\n",
    "            if dtype is not None and arr.dtype != np.dtype(dtype):\n",
    "                # memmap dtype conversion would copy; better to keep original\n",
    "                pass\n",
    "            self.arrs.append(arr)\n",
    "            self.cum.append(self.cum[-1] + n)\n",
    "\n",
    "        self.cum = np.array(self.cum, dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.cum[-1])\n",
    "\n",
    "    def _locate(self, idx: int):\n",
    "        # shard_id such that cum[shard_id] <= idx < cum[shard_id+1]\n",
    "        sid = int(np.searchsorted(self.cum, idx, side=\"right\") - 1)\n",
    "        local = int(idx - self.cum[sid])\n",
    "        return sid, local\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sid, local = self._locate(idx)\n",
    "        return self.arrs[sid][local]\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Split indices (90/5/5)\n",
    "# --------------------------\n",
    "def make_split_indices(N: int, seed: int = 0, frac_train=0.90, frac_val=0.05, frac_test=0.05):\n",
    "    assert abs(frac_train + frac_val + frac_test - 1.0) < 1e-9\n",
    "    rng = np.random.default_rng(seed)\n",
    "    perm = rng.permutation(N)\n",
    "    n_train = int(N * frac_train)\n",
    "    n_val = int(N * frac_val)\n",
    "    idx_train = perm[:n_train]\n",
    "    idx_val   = perm[n_train:n_train+n_val]\n",
    "    idx_test  = perm[n_train+n_val:]\n",
    "    return idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Minimal normalizers (fast & safe)\n",
    "# --------------------------\n",
    "class XChannelStandardizer:\n",
    "    \"\"\"\n",
    "    log1p then per-channel standardize, for inputs shaped (T, C).\n",
    "    \"\"\"\n",
    "    def __init__(self, log1p=True, eps=1e-6):\n",
    "        self.log1p = log1p\n",
    "        self.eps = eps\n",
    "        self.mean = None  # (C,)\n",
    "        self.std = None   # (C,)\n",
    "\n",
    "    def fit(self, X_TxC: np.ndarray):\n",
    "        # X_TxC stacked over many sequences: shape (N*T, C) or (N, T, C)\n",
    "        if X_TxC.ndim == 3:\n",
    "            X = X_TxC.reshape(-1, X_TxC.shape[-1])\n",
    "        else:\n",
    "            X = X_TxC\n",
    "        if self.log1p:\n",
    "            X = np.log1p(np.clip(X, 0.0, None))\n",
    "        self.mean = X.mean(axis=0)\n",
    "        self.std = X.std(axis=0)\n",
    "        self.std = np.maximum(self.std, self.eps)\n",
    "\n",
    "    def transform_one(self, x_TxC: np.ndarray) -> np.ndarray:\n",
    "        x = x_TxC.astype(np.float32, copy=False)\n",
    "        if self.log1p:\n",
    "            x = np.log1p(np.clip(x, 0.0, None))\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "\n",
    "class MaskedLogParamStandardizer:\n",
    "    \"\"\"\n",
    "    For positive parameters: log(p) then per-dim standardize using only masked entries.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-12):\n",
    "        self.eps = eps\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit(self, Y: np.ndarray, M: np.ndarray):\n",
    "        # Y: (N, D), M: (N, D) with 1 for used\n",
    "        Y = Y.astype(np.float64)\n",
    "        M = M.astype(np.float64)\n",
    "        D = Y.shape[1]\n",
    "        mean = np.zeros((D,), dtype=np.float64)\n",
    "        var = np.ones((D,), dtype=np.float64)\n",
    "\n",
    "        for d in range(D):\n",
    "            mask = M[:, d] > 0.5\n",
    "            if mask.any():\n",
    "                vals = np.log(np.clip(Y[mask, d], self.eps, None))\n",
    "                mean[d] = vals.mean()\n",
    "                var[d] = vals.var() + 1e-6\n",
    "            else:\n",
    "                mean[d] = 0.0\n",
    "                var[d] = 1.0\n",
    "        self.mean = mean.astype(np.float32)\n",
    "        self.std = np.sqrt(var).astype(np.float32)\n",
    "\n",
    "    def transform_one(self, y: np.ndarray, m: np.ndarray) -> np.ndarray:\n",
    "        y = y.astype(np.float32, copy=False)\n",
    "        m = m.astype(np.float32, copy=False)\n",
    "        out = np.zeros_like(y, dtype=np.float32)\n",
    "        used = m > 0.5\n",
    "        out[used] = (np.log(np.clip(y[used], self.eps, None)) - self.mean[used]) / self.std[used]\n",
    "        # unused stay 0\n",
    "        return out\n",
    "\n",
    "    def inverse_one(self, y_norm: np.ndarray, m: np.ndarray) -> np.ndarray:\n",
    "        # return params in original space (exp)\n",
    "        y_norm = y_norm.astype(np.float32, copy=False)\n",
    "        m = m.astype(np.float32, copy=False)\n",
    "        out = np.zeros_like(y_norm, dtype=np.float32)\n",
    "        used = m > 0.5\n",
    "        out[used] = np.exp(y_norm[used] * self.std[used] + self.mean[used])\n",
    "        return out\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Dataset wrapping sharded arrays\n",
    "# --------------------------\n",
    "class ShardedMultiTaskSeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x: torch.float32 (T, C)\n",
    "      y_cls: torch.long ()\n",
    "      y_reg: torch.float32 (D,)\n",
    "      y_mask: torch.float32 (D,)\n",
    "    \"\"\"\n",
    "    def __init__(self, X_2xT: ShardedMemmap, y_cls: ShardedMemmap, y_reg: ShardedMemmap, y_mask: ShardedMemmap,\n",
    "                 x_norm: XChannelStandardizer, y_norm: MaskedLogParamStandardizer):\n",
    "        self.X_2xT = X_2xT\n",
    "        self.y_cls = y_cls\n",
    "        self.y_reg = y_reg\n",
    "        self.y_mask = y_mask\n",
    "        self.x_norm = x_norm\n",
    "        self.y_norm = y_norm\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_2xT)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_2xT = self.X_2xT[idx]                  # (2, T)\n",
    "        x_Tx2 = np.transpose(x_2xT, (1, 0))      # (T, 2)\n",
    "        x_Tx2 = self.x_norm.transform_one(x_Tx2)\n",
    "\n",
    "        yc = int(self.y_cls[idx])\n",
    "        yr = np.array(self.y_reg[idx], dtype=np.float32)\n",
    "        ym = np.array(self.y_mask[idx], dtype=np.float32)\n",
    "        yrn = self.y_norm.transform_one(yr, ym)\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x_Tx2).float(),\n",
    "            torch.tensor(yc, dtype=torch.long),\n",
    "            torch.from_numpy(yrn).float(),\n",
    "            torch.from_numpy(ym).float(),\n",
    "        )\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Helper: fit normalizers on a subset of TRAIN indices\n",
    "# --------------------------\n",
    "def fit_normalizers(dataset_kind: str,\n",
    "                    shard_dirs,\n",
    "                    train_idx: np.ndarray,\n",
    "                    fit_n: int = 200_000,\n",
    "                    seed: int = 0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    fit_n = min(fit_n, len(train_idx))\n",
    "    sel = rng.choice(train_idx, size=fit_n, replace=False)\n",
    "\n",
    "    if dataset_kind == \"pk\":\n",
    "        X = ShardedMemmap(shard_dirs, \"X_pk.npy\")\n",
    "        Y = ShardedMemmap(shard_dirs, \"y_pk_reg.npy\")\n",
    "        M = ShardedMemmap(shard_dirs, \"y_pk_mask.npy\")\n",
    "    else:\n",
    "        X = ShardedMemmap(shard_dirs, \"X_pd.npy\")\n",
    "        Y = ShardedMemmap(shard_dirs, \"y_pd_reg.npy\")\n",
    "        M = ShardedMemmap(shard_dirs, \"y_pd_mask.npy\")\n",
    "\n",
    "    # Fit X normalizer: stack (T,2) into (N*T,2) incrementally\n",
    "    # (Avoid building an enormous array at once)\n",
    "    xs = []\n",
    "    for i in sel[:min(50_000, fit_n)]:  # pragmatic cap to keep RAM sane\n",
    "        x_Tx2 = np.transpose(X[int(i)], (1, 0)).astype(np.float32, copy=False)\n",
    "        xs.append(x_Tx2)\n",
    "    xs = np.stack(xs, axis=0)  # (n, T, 2)\n",
    "    x_norm = XChannelStandardizer(log1p=True)\n",
    "    x_norm.fit(xs)\n",
    "\n",
    "    # Fit Y normalizer\n",
    "    ys = np.stack([np.array(Y[int(i)], dtype=np.float32) for i in sel[:min(200_000, fit_n)]], axis=0)\n",
    "    ms = np.stack([np.array(M[int(i)], dtype=np.float32) for i in sel[:min(200_000, fit_n)]], axis=0)\n",
    "    y_norm = MaskedLogParamStandardizer()\n",
    "    y_norm.fit(ys, ms)\n",
    "\n",
    "    return x_norm, y_norm\n",
    "\n",
    "\n",
    "\n",
    "BASE = \"./dataset\"\n",
    "shards = discover_mpirank_shards(BASE)\n",
    "print(\"Found shards:\", len(shards), \"first/last:\", shards[0], shards[-1])\n",
    "\n",
    "# Load sharded arrays (memmaps)\n",
    "X_pk = ShardedMemmap(shards, \"X_pk.npy\")\n",
    "y_pk_cls = ShardedMemmap(shards, \"y_pk_cls.npy\")\n",
    "y_pk_reg = ShardedMemmap(shards, \"y_pk_reg.npy\")\n",
    "y_pk_mask = ShardedMemmap(shards, \"y_pk_mask.npy\")\n",
    "\n",
    "X_pd = ShardedMemmap(shards, \"X_pd.npy\")\n",
    "y_pd_cls = ShardedMemmap(shards, \"y_pd_cls.npy\")\n",
    "y_pd_reg = ShardedMemmap(shards, \"y_pd_reg.npy\")\n",
    "y_pd_mask = ShardedMemmap(shards, \"y_pd_mask.npy\")\n",
    "\n",
    "N = len(X_pk)\n",
    "print(\"Total samples:\", N)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c75be208-d485-4914-9284-0ca6dc9defe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices once and reuse for PK+PD\n",
    "idx_tr, idx_va, idx_te = make_split_indices(N, seed=123, frac_train=0.90, frac_val=0.05, frac_test=0.05)\n",
    "\n",
    "# Save split indices for reproducibility\n",
    "np.savez(os.path.join(BASE, \"splits_90_5_5_seed123.npz\"),\n",
    "         idx_tr=idx_tr, idx_va=idx_va, idx_te=idx_te)\n",
    "\n",
    "# Fit normalizers (train-only) for PK + PD\n",
    "xnorm_pk, ynorm_pk = fit_normalizers(\"pk\", shards, idx_tr, fit_n=200_000, seed=1)\n",
    "xnorm_pd, ynorm_pd = fit_normalizers(\"pd\", shards, idx_tr, fit_n=200_000, seed=2)\n",
    "\n",
    "ds_pk = ShardedMultiTaskSeqDataset(X_pk, y_pk_cls, y_pk_reg, y_pk_mask, xnorm_pk, ynorm_pk)\n",
    "ds_pd = ShardedMultiTaskSeqDataset(X_pd, y_pd_cls, y_pd_reg, y_pd_mask, xnorm_pd, ynorm_pd)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def masked_mse(pred, target, mask):\n",
    "    # pred/target: (B, D), mask: (B, D)\n",
    "    diff2 = (pred - target) ** 2\n",
    "    diff2 = diff2 * mask\n",
    "    denom = mask.sum().clamp_min(1.0)\n",
    "    return diff2.sum() / denom\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, times_t, lambda_reg=1.0):\n",
    "    model.eval()\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_n = 0\n",
    "    total_reg = 0.0\n",
    "\n",
    "    for x, y_cls, y_reg, y_mask in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y_cls = y_cls.to(device, non_blocking=True)\n",
    "        y_reg = y_reg.to(device, non_blocking=True)\n",
    "        y_mask = y_mask.to(device, non_blocking=True)\n",
    "\n",
    "        # IMPORTANT: pass times\n",
    "        logits, yhat_reg = model(x, times_t)\n",
    "\n",
    "        loss_cls = ce(logits, y_cls)\n",
    "        loss_reg = masked_mse(yhat_reg, y_reg, y_mask)\n",
    "        loss = loss_cls + lambda_reg * loss_reg\n",
    "\n",
    "        total_loss += float(loss.item()) * x.size(0)\n",
    "        total_reg  += float(loss_reg.item()) * x.size(0)\n",
    "\n",
    "        pred = logits.argmax(dim=1)\n",
    "        total_correct += int((pred == y_cls).sum().item())\n",
    "        total_n += int(x.size(0))\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / max(total_n, 1),\n",
    "        \"acc\": total_correct / max(total_n, 1),\n",
    "        \"reg_mse_masked\": total_reg / max(total_n, 1),\n",
    "        \"n\": total_n\n",
    "    }\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "def train_one_config(\n",
    "    run_name: str,\n",
    "    dataset,\n",
    "    idx_tr,\n",
    "    idx_va,\n",
    "    num_classes: int,\n",
    "    reg_dim: int,\n",
    "    device,\n",
    "    out_dir: str,\n",
    "    cfg: dict,\n",
    "    times_t,\n",
    "    resume_training: bool = False,\n",
    "    resume_ckpt_path: str | None = None,\n",
    "):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ckpt_dir = out_dir / \"checkpoints\"\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_path = out_dir / f\"{run_name}_best.pt\"\n",
    "    log_path  = out_dir / f\"{run_name}_log.jsonl\"\n",
    "\n",
    "    def _as_list(x):\n",
    "        return np.asarray(x, dtype=np.float32).tolist()\n",
    "\n",
    "    # If dataset might be a Subset, unwrap it\n",
    "    base_ds = dataset.dataset if hasattr(dataset, \"dataset\") else dataset\n",
    "    \n",
    "    x_norm_obj = getattr(base_ds, \"x_norm\", None)\n",
    "    y_norm_obj = getattr(base_ds, \"y_norm\", None)\n",
    "    # ----------------------------\n",
    "    # DataLoaders (same as before)\n",
    "    # ----------------------------\n",
    "    dl_tr = DataLoader(\n",
    "        Subset(dataset, idx_tr),\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=cfg.get(\"num_workers\", 2),\n",
    "        pin_memory=True,\n",
    "        persistent_workers=(cfg.get(\"num_workers\", 2) > 0),\n",
    "    )\n",
    "    dl_va = DataLoader(\n",
    "        Subset(dataset, idx_va),\n",
    "        batch_size=cfg.get(\"eval_batch_size\", 512),\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.get(\"num_workers\", 2),\n",
    "        pin_memory=True,\n",
    "        persistent_workers=(cfg.get(\"num_workers\", 2) > 0),\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # Model\n",
    "    # ----------------------------\n",
    "    model = CP.MultiTaskTransformer(\n",
    "        input_dim=2,\n",
    "        num_classes=num_classes,\n",
    "        reg_dim=reg_dim,\n",
    "        d_model=cfg[\"d_model\"],\n",
    "        nhead=cfg[\"nhead\"],\n",
    "        num_layers=cfg[\"num_layers\"],\n",
    "        dim_feedforward=cfg[\"dim_feedforward\"],\n",
    "        dropout=cfg[\"dropout\"],\n",
    "    ).to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n",
    "\n",
    "    # schedule\n",
    "    steps_per_epoch = len(dl_tr)\n",
    "    total_steps = steps_per_epoch * cfg[\"epochs\"]\n",
    "    warmup_steps = int(total_steps * cfg.get(\"warmup_frac\", 0.05))\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return (step + 1) / max(1, warmup_steps)\n",
    "        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "    sched = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lr_lambda)\n",
    "\n",
    "    ce = nn.CrossEntropyLoss(label_smoothing=cfg.get(\"label_smoothing\", 0.0))\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.get(\"amp\", True))\n",
    "\n",
    "    save_every = int(cfg.get(\"save_every_epochs\", 0))   # 0 disables\n",
    "    patience = int(cfg.get(\"patience\", 0))              # 0 disables\n",
    "    min_delta = float(cfg.get(\"min_delta\", 0.0))\n",
    "\n",
    "    # ----------------------------\n",
    "    # Resume state (NEW)\n",
    "    # ----------------------------\n",
    "    start_epoch = 0\n",
    "    step = 0\n",
    "    best = {\"val_loss\": float(\"inf\")}\n",
    "    best_epoch = -1\n",
    "    bad_epochs = 0\n",
    "\n",
    "    if resume_training:\n",
    "        if resume_ckpt_path is None:\n",
    "            # default: resume from best if exists\n",
    "            resume_ckpt_path = str(best_path) if best_path.exists() else None\n",
    "\n",
    "        if resume_ckpt_path is None or not os.path.exists(resume_ckpt_path):\n",
    "            raise FileNotFoundError(f\"resume_training=True but checkpoint not found: {resume_ckpt_path}\")\n",
    "\n",
    "        ckpt = torch.load(resume_ckpt_path, map_location=device)\n",
    "\n",
    "        # restore weights + optimizer/scheduler/scaler\n",
    "        model.load_state_dict(ckpt[\"model_state\"])\n",
    "        if \"opt_state\" in ckpt and ckpt[\"opt_state\"] is not None:\n",
    "            opt.load_state_dict(ckpt[\"opt_state\"])\n",
    "        if \"sched_state\" in ckpt and ckpt[\"sched_state\"] is not None:\n",
    "            sched.load_state_dict(ckpt[\"sched_state\"])\n",
    "        if \"scaler_state\" in ckpt and ckpt[\"scaler_state\"] is not None and scaler is not None:\n",
    "            scaler.load_state_dict(ckpt[\"scaler_state\"])\n",
    "\n",
    "        # restore counters\n",
    "        start_epoch = int(ckpt.get(\"epoch\", -1)) + 1\n",
    "        step = int(ckpt.get(\"step\", 0))\n",
    "\n",
    "        # restore best tracking if present\n",
    "        if \"best_val\" in ckpt and ckpt[\"best_val\"] is not None:\n",
    "            best = {\"val_loss\": ckpt[\"best_val\"][\"loss\"], **ckpt[\"best_val\"]}\n",
    "            best_epoch = int(ckpt.get(\"epoch\", -1))\n",
    "            bad_epochs = 0  # reset; you can also load this if you saved it\n",
    "\n",
    "        print(f\"[{run_name}] Resuming from {resume_ckpt_path}\")\n",
    "        print(f\"  start_epoch={start_epoch}, step={step}, best_val_loss={best['val_loss']:.6f}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Training loop (same as before, but start_epoch)\n",
    "    # ----------------------------\n",
    "    for epoch in range(start_epoch, cfg[\"epochs\"]):\n",
    "        model.train()\n",
    "        for x, y_cls, y_reg, y_mask in dl_tr:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y_cls = y_cls.to(device, non_blocking=True)\n",
    "            y_reg = y_reg.to(device, non_blocking=True)\n",
    "            y_mask = y_mask.to(device, non_blocking=True)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=cfg.get(\"amp\", True)):\n",
    "                logits, yhat_reg = model(x, times_t)\n",
    "                loss_cls = ce(logits, y_cls)\n",
    "                loss_reg = masked_mse(yhat_reg, y_reg, y_mask)\n",
    "                loss = loss_cls + cfg[\"lambda_reg\"] * loss_reg\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            if cfg.get(\"grad_clip\", 0.0) > 0:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg[\"grad_clip\"])\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            sched.step()\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        # ---- validate\n",
    "        val_metrics = evaluate(model, dl_va, device, times_t, lambda_reg=cfg[\"lambda_reg\"])\n",
    "\n",
    "        # ---- log append\n",
    "        rec = {\"epoch\": epoch, \"step\": step, \"cfg\": cfg, \"val\": val_metrics}\n",
    "        with open(log_path, \"a\") as f:\n",
    "            f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "        # ---- periodic checkpoint\n",
    "        if save_every and ((epoch + 1) % save_every == 0):\n",
    "            ckpt_path = ckpt_dir / f\"{run_name}_epoch{epoch+1:04d}.pt\"\n",
    "            torch.save({\n",
    "                \"run_name\": run_name,\n",
    "                \"epoch\": epoch,\n",
    "                \"step\": step,\n",
    "                \"cfg\": cfg,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"opt_state\": opt.state_dict(),\n",
    "                \"sched_state\": sched.state_dict(),\n",
    "                \"scaler_state\": scaler.state_dict() if scaler is not None else None,\n",
    "                \"val\": val_metrics,\n",
    "                \"best_val\": best if best[\"val_loss\"] < float(\"inf\") else None,\n",
    "                \"x_norm\": (None if x_norm_obj is None else {\n",
    "                    \"mean\": _as_list(x_norm_obj.mean),\n",
    "                    \"std\":  _as_list(x_norm_obj.std),\n",
    "                    \"log1p\": bool(getattr(x_norm_obj, \"log1p\", True)),\n",
    "                }),\n",
    "                \"y_norm\": (None if y_norm_obj is None else {\n",
    "                    \"mean\": _as_list(y_norm_obj.mean),\n",
    "                    \"std\":  _as_list(y_norm_obj.std),\n",
    "            }),\n",
    "            }, ckpt_path)\n",
    "\n",
    "        # ---- best checkpoint + early stopping\n",
    "        improved = (val_metrics[\"loss\"] < best[\"val_loss\"] - min_delta)\n",
    "        if improved:\n",
    "            best = {\"val_loss\": val_metrics[\"loss\"], **val_metrics}\n",
    "            best_epoch = epoch\n",
    "            bad_epochs = 0\n",
    "            torch.save({\n",
    "                \"run_name\": run_name,\n",
    "                \"epoch\": epoch,\n",
    "                \"step\": step,\n",
    "                \"cfg\": cfg,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"opt_state\": opt.state_dict(),\n",
    "                \"sched_state\": sched.state_dict(),\n",
    "                \"scaler_state\": scaler.state_dict() if scaler is not None else None,\n",
    "                \"best_val\": best,\n",
    "                # Save normalizers if present (so inference later is consistent)\n",
    "                \"x_norm\": (None if x_norm_obj is None else {\n",
    "                    \"mean\": _as_list(x_norm_obj.mean),\n",
    "                    \"std\":  _as_list(x_norm_obj.std),\n",
    "                    \"log1p\": bool(getattr(x_norm_obj, \"log1p\", True)),\n",
    "                }),\n",
    "                \"y_norm\": (None if y_norm_obj is None else {\n",
    "                    \"mean\": _as_list(y_norm_obj.mean),\n",
    "                    \"std\":  _as_list(y_norm_obj.std),\n",
    "                }),\n",
    "\n",
    "            }, best_path)\n",
    "        else:\n",
    "            if patience:\n",
    "                bad_epochs += 1\n",
    "                if bad_epochs >= patience:\n",
    "                    print(f\"[{run_name}] Early stop at epoch={epoch+1} (best epoch={best_epoch+1}, best val_loss={best['val_loss']:.4f})\")\n",
    "                    break\n",
    "\n",
    "        pct = 100.0 * (epoch + 1) / cfg[\"epochs\"]\n",
    "        print(f\"[{run_name}] epoch={epoch+1}/{cfg['epochs']} ({pct:.1f}%) \"\n",
    "              f\"val_loss={val_metrics['loss']:.4f} acc={val_metrics['acc']:.4f} reg={val_metrics['reg_mse_masked']:.4f}\")\n",
    "\n",
    "    return best, str(best_path)\n",
    "\n",
    "\n",
    "def hyperparam_sweep(task_name: str,\n",
    "                     dataset: Dataset,\n",
    "                     idx_tr: np.ndarray,\n",
    "                     idx_va: np.ndarray,\n",
    "                     num_classes: int,\n",
    "                     reg_dim: int,\n",
    "                     device,\n",
    "                     out_dir: str,\n",
    "                     configs: list,\n",
    "                     times_t):\n",
    "    results = []\n",
    "    for k, cfg in enumerate(configs):\n",
    "        run_name = f\"{task_name}_trial{k:03d}\"\n",
    "        best, ckpt_path = train_one_config(\n",
    "            run_name=run_name,\n",
    "            dataset=dataset,\n",
    "            idx_tr=idx_tr,\n",
    "            idx_va=idx_va,\n",
    "            num_classes=num_classes,\n",
    "            reg_dim=reg_dim,\n",
    "            device=device,\n",
    "            out_dir=out_dir,\n",
    "            cfg=cfg,\n",
    "            times_t=times_t\n",
    "        )\n",
    "        results.append({\"run\": run_name, \"best\": best, \"ckpt\": ckpt_path, \"cfg\": cfg})\n",
    "\n",
    "    # write summary\n",
    "    summary_path = os.path.join(out_dir, f\"{task_name}_sweep_summary.json\")\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "809f08df-a9e7-4e73-9662-54f5069d9750",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1613139b-1298-45fd-af08-892c25891cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "configs = [\n",
    "    dict(d_model=64,  nhead=4, num_layers=4, dim_feedforward=256, dropout=0.10,\n",
    "         lr=3e-4, weight_decay=1e-2, lambda_reg=1.0, batch_size=256, epochs=N_epochs,\n",
    "         warmup_frac=0.05, amp=True, grad_clip=1.0, num_workers=2),\n",
    "\n",
    "    dict(d_model=128, nhead=8, num_layers=4, dim_feedforward=512, dropout=0.10,\n",
    "         lr=3e-4, weight_decay=1e-2, lambda_reg=1.0, batch_size=256, epochs=N_epochs,\n",
    "         warmup_frac=0.05, amp=True, grad_clip=1.0, num_workers=2),\n",
    "\n",
    "    dict(d_model=128, nhead=8, num_layers=6, dim_feedforward=512, dropout=0.15,\n",
    "         lr=2e-4, weight_decay=2e-2, lambda_reg=0.7, batch_size=256, epochs=N_epochs,\n",
    "         warmup_frac=0.08, amp=True, grad_clip=1.0, num_workers=2),\n",
    "\n",
    "    dict(d_model=192, nhead=8, num_layers=6, dim_feedforward=768, dropout=0.15,\n",
    "         lr=2e-4, weight_decay=1e-2, lambda_reg=1.2, batch_size=192, epochs=N_epochs,\n",
    "         warmup_frac=0.08, amp=True, grad_clip=1.0, num_workers=2),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f551b050-acc7-4244-b3aa-c44e2fa473a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_times_t = torch.tensor(TG.PK_TIMES, dtype=torch.float32, device=device)  # (39,)\n",
    "pd_times_t = torch.tensor(TG.PD_TIMES, dtype=torch.float32, device=device)  # (25,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60220744-7f97-405b-84ad-81984af3bc3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/b/by1997/.conda/envs/rl_agent/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PK_trial000] epoch=1/3 (33.3%) val_loss=1.6950 acc=0.5129 reg=0.5235\n",
      "[PK_trial000] epoch=2/3 (66.7%) val_loss=1.5962 acc=0.5337 reg=0.4968\n",
      "[PK_trial000] epoch=3/3 (100.0%) val_loss=1.5877 acc=0.5359 reg=0.4940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/b/by1997/.conda/envs/rl_agent/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PK_trial001] epoch=1/3 (33.3%) val_loss=1.5981 acc=0.5355 reg=0.4997\n",
      "[PK_trial001] epoch=2/3 (66.7%) val_loss=1.5480 acc=0.5465 reg=0.4793\n",
      "[PK_trial001] epoch=3/3 (100.0%) val_loss=1.5243 acc=0.5530 reg=0.4725\n",
      "[PK_trial002] epoch=1/3 (33.3%) val_loss=1.4663 acc=0.5318 reg=0.5045\n",
      "[PK_trial002] epoch=2/3 (66.7%) val_loss=1.4344 acc=0.5391 reg=0.4935\n",
      "[PK_trial002] epoch=3/3 (100.0%) val_loss=1.4108 acc=0.5457 reg=0.4850\n",
      "[PK_trial003] epoch=1/3 (33.3%) val_loss=1.7200 acc=0.5329 reg=0.5004\n",
      "[PK_trial003] epoch=2/3 (66.7%) val_loss=1.6352 acc=0.5498 reg=0.4766\n",
      "[PK_trial003] epoch=3/3 (100.0%) val_loss=1.6111 acc=0.5563 reg=0.4711\n",
      "[PD_trial000] epoch=3/3 (100.0%) val_loss=2.9992 acc=0.1249 reg=0.7229\n",
      "[PD_trial001] epoch=1/3 (33.3%) val_loss=3.0031 acc=0.1234 reg=0.7234\n",
      "[PD_trial001] epoch=2/3 (66.7%) val_loss=2.9945 acc=0.1275 reg=0.7217\n",
      "[PD_trial001] epoch=3/3 (100.0%) val_loss=2.9897 acc=0.1296 reg=0.7208\n",
      "[PD_trial002] epoch=1/3 (33.3%) val_loss=2.7873 acc=0.1223 reg=0.7240\n",
      "[PD_trial002] epoch=2/3 (66.7%) val_loss=2.7832 acc=0.1252 reg=0.7234\n",
      "[PD_trial002] epoch=3/3 (100.0%) val_loss=2.7806 acc=0.1266 reg=0.7227\n",
      "[PD_trial003] epoch=1/3 (33.3%) val_loss=3.1521 acc=0.1185 reg=0.7237\n",
      "[PD_trial003] epoch=2/3 (66.7%) val_loss=3.1413 acc=0.1261 reg=0.7222\n",
      "[PD_trial003] epoch=3/3 (100.0%) val_loss=3.1369 acc=0.1291 reg=0.7213\n",
      "Sweep finished.\n"
     ]
    }
   ],
   "source": [
    "OUT = \"/pscratch/sd/b/by1997/pkpd/training_runs/sweep1\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "# PK sweep\n",
    "pk_results = hyperparam_sweep(\n",
    "    task_name=\"PK\",\n",
    "    dataset=ds_pk,\n",
    "    idx_tr=idx_tr,\n",
    "    idx_va=idx_va,\n",
    "    num_classes=10,\n",
    "    reg_dim=TG.PK_PARAM_DIM,\n",
    "    device=device,\n",
    "    out_dir=os.path.join(OUT, \"pk\"),\n",
    "    configs=configs,\n",
    "    times_t=pk_times_t\n",
    ")\n",
    "\n",
    "# PD sweep\n",
    "pd_results = hyperparam_sweep(\n",
    "    task_name=\"PD\",\n",
    "    dataset=ds_pd,\n",
    "    idx_tr=idx_tr,\n",
    "    idx_va=idx_va,\n",
    "    num_classes=10,\n",
    "    reg_dim=TG.PD_PARAM_DIM,\n",
    "    device=device,\n",
    "    out_dir=os.path.join(OUT, \"pd\"),\n",
    "    configs=configs,\n",
    "    times_t=pd_times_t\n",
    ")\n",
    "\n",
    "print(\"Sweep finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f70709d1-cd51-436e-8300-4b7cc87e801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PK_trial002 val_loss: 1.410815221710205 val_acc: 0.545748 reg_mse: 0.4849929124298096\n",
      "PK_trial001 val_loss: 1.5243376971664429 val_acc: 0.55303 reg_mse: 0.47247732859611513\n",
      "PK_trial000 val_loss: 1.5876962135772705 val_acc: 0.53591 reg_mse: 0.49399089193344115\n",
      "PK_trial003 val_loss: 1.6110592265319825 val_acc: 0.556278 reg_mse: 0.4710990595493317\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "summary_path = \"/pscratch/sd/b/by1997/pkpd/training_runs/sweep1/pk/PK_sweep_summary.json\"\n",
    "with open(summary_path, \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# sort by val loss\n",
    "results_sorted = sorted(results, key=lambda r: r[\"best\"][\"loss\"])\n",
    "for r in results_sorted[:10]:\n",
    "    print(r[\"run\"], \"val_loss:\", r[\"best\"][\"loss\"], \"val_acc:\", r[\"best\"][\"acc\"], \"reg_mse:\", r[\"best\"][\"reg_mse_masked\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f25a34a6-352f-4241-86fd-7d551972026c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min val_loss: 1.5876962135772705 at epoch 2\n",
      "max val_acc: 0.53591 at epoch 2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "log_path = \"/pscratch/sd/b/by1997/pkpd/training_runs/sweep1/pk/PK_trial000_log.jsonl\"\n",
    "\n",
    "epochs, vloss, vacc, vreg = [], [], [], []\n",
    "with open(log_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        rec = json.loads(line)\n",
    "        epochs.append(rec[\"epoch\"])\n",
    "        vloss.append(rec[\"val\"][\"loss\"])\n",
    "        vacc.append(rec[\"val\"][\"acc\"])\n",
    "        vreg.append(rec[\"val\"][\"reg_mse_masked\"])\n",
    "\n",
    "print(\"min val_loss:\", np.min(vloss), \"at epoch\", epochs[int(np.argmin(vloss))])\n",
    "print(\"max val_acc:\", np.max(vacc), \"at epoch\", epochs[int(np.argmax(vacc))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0590bdf3-d4c9-4351-83d2-6eedff779f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best balanced: PK_trial003 {'val_loss': 1.6110592265319825, 'loss': 1.6110592265319825, 'acc': 0.556278, 'reg_mse_masked': 0.4710990595493317, 'n': 500000}\n"
     ]
    }
   ],
   "source": [
    "def score(r, w_acc=1.0, w_reg=0.2):\n",
    "    # higher is better: acc up, reg down\n",
    "    return w_acc * r[\"best\"][\"acc\"] - w_reg * r[\"best\"][\"reg_mse_masked\"]\n",
    "\n",
    "best_balanced = max(results, key=lambda r: score(r))\n",
    "print(\"Best balanced:\", best_balanced[\"run\"], best_balanced[\"best\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f250c487-4447-467f-86e0-c37dfcd10c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run': 'PK_trial003',\n",
       " 'best': {'val_loss': 1.6110592265319825,\n",
       "  'loss': 1.6110592265319825,\n",
       "  'acc': 0.556278,\n",
       "  'reg_mse_masked': 0.4710990595493317,\n",
       "  'n': 500000},\n",
       " 'ckpt': '/pscratch/sd/b/by1997/pkpd/training_runs/sweep1/pk/PK_trial003_best.pt',\n",
       " 'cfg': {'d_model': 192,\n",
       "  'nhead': 8,\n",
       "  'num_layers': 6,\n",
       "  'dim_feedforward': 768,\n",
       "  'dropout': 0.15,\n",
       "  'lr': 0.0002,\n",
       "  'weight_decay': 0.01,\n",
       "  'lambda_reg': 1.2,\n",
       "  'batch_size': 192,\n",
       "  'epochs': 3,\n",
       "  'warmup_frac': 0.08,\n",
       "  'amp': True,\n",
       "  'grad_clip': 1.0,\n",
       "  'num_workers': 2}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2174a67-5e41-46ad-b6e9-236443545ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def write_best_cfg_json(results, out_path: str):\n",
    "    # results: list of dicts from hyperparam_sweep\n",
    "    best_item = min(results, key=lambda r: r[\"best\"][\"val_loss\"])\n",
    "    cfg = dict(best_item[\"cfg\"])  # copy\n",
    "\n",
    "    # Optionally: override epochs for long training here\n",
    "    # cfg[\"epochs\"] = 400\n",
    "    # cfg[\"save_every_epochs\"] = 10\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(cfg, f, indent=2)\n",
    "\n",
    "    print(\"Wrote cfg to:\", out_path)\n",
    "    print(\"Best run:\", best_item[\"run\"])\n",
    "    print(\"Best val:\", best_item[\"best\"])\n",
    "    return cfg, best_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f22a20a-1e58-4dae-93e5-79d75258112d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote cfg to: /pscratch/sd/b/by1997/pkpd/configs/cfg_final_pk.json\n",
      "Best run: PK_trial002\n",
      "Best val: {'val_loss': 1.410815221710205, 'loss': 1.410815221710205, 'acc': 0.545748, 'reg_mse_masked': 0.4849929124298096, 'n': 500000}\n",
      "Wrote cfg to: /pscratch/sd/b/by1997/pkpd/configs/cfg_final_pd.json\n",
      "Best run: PD_trial002\n",
      "Best val: {'val_loss': 2.7805983345947265, 'loss': 2.7805983345947265, 'acc': 0.12658, 'reg_mse_masked': 0.7227154406700135, 'n': 500000}\n"
     ]
    }
   ],
   "source": [
    "cfg_pk, best_pk = write_best_cfg_json(\n",
    "    pk_results,\n",
    "    out_path=\"/pscratch/sd/b/by1997/pkpd/configs/cfg_final_pk.json\"\n",
    ")\n",
    "\n",
    "cfg_pd, best_pd = write_best_cfg_json(\n",
    "    pd_results,\n",
    "    out_path=\"/pscratch/sd/b/by1997/pkpd/configs/cfg_final_pd.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "005c8bf4-1db2-468a-85f8-da575dda1fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_pk[\"epochs\"] = 150\n",
    "cfg_pk[\"save_every_epochs\"] = 10   # better than 50 if jobs die every 6h\n",
    "cfg_pk[\"eval_batch_size\"] = 1024\n",
    "with open(\"/pscratch/sd/b/by1997/pkpd/configs/cfg_final_pk.json\",\"w\") as f:\n",
    "    json.dump(cfg_pk, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7f0c269-f9d8-432a-bd4c-251d62375791",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_pd[\"epochs\"] = 150\n",
    "cfg_pd[\"save_every_epochs\"] = 10   # better than 50 if jobs die every 6h\n",
    "cfg_pd[\"eval_batch_size\"] = 1024\n",
    "with open(\"/pscratch/sd/b/by1997/pkpd/configs/cfg_final_pd.json\",\"w\") as f:\n",
    "    json.dump(cfg_pd, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e907d62-f6b8-4305-a963-feb1e0dbcf2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m pk_times_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(TG\u001b[38;5;241m.\u001b[39mPK_TIMES, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 26\u001b[0m best_long, best_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPK_best_long\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_pk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43midx_tr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43midx_va\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx_va\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreg_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPK_PARAM_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUT_LONG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_long\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimes_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpk_times_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# <-- from scratch\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_ckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest val metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_long)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest checkpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_ckpt_path)\n",
      "Cell \u001b[0;32mIn[2], line 215\u001b[0m, in \u001b[0;36mtrain_one_config\u001b[0;34m(run_name, dataset, idx_tr, idx_va, num_classes, reg_dim, device, out_dir, cfg, times_t, resume_training, resume_ckpt_path)\u001b[0m\n\u001b[1;32m    212\u001b[0m     loss_reg \u001b[38;5;241m=\u001b[39m masked_mse(yhat_reg, y_reg, y_mask)\n\u001b[1;32m    213\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_cls \u001b[38;5;241m+\u001b[39m cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda_reg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m loss_reg\n\u001b[0;32m--> 215\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_clip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    217\u001b[0m     scaler\u001b[38;5;241m.\u001b[39munscale_(opt)\n",
      "File \u001b[0;32m~/.conda/envs/rl_agent/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/rl_agent/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/rl_agent/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, copy\n",
    "import torch\n",
    "import TrajectoryGenerator as TG\n",
    "\n",
    "# Choose your best cfg from the sweep summary (or hardcode it)\n",
    "cfg_long = copy.deepcopy(best_balanced[\"cfg\"])   # or best_by_acc[\"cfg\"] etc.\n",
    "\n",
    "# Make it \"long\"\n",
    "cfg_long[\"epochs\"] = 300\n",
    "cfg_long[\"save_every_epochs\"] = 50   # periodic checkpoints\n",
    "cfg_long[\"patience\"] = 20            # early stop if no improvement for 20 epochs\n",
    "cfg_long[\"min_delta\"] = 1e-4\n",
    "cfg_long[\"eval_batch_size\"] = 1024   # faster validation\n",
    "cfg_long[\"amp\"] = True\n",
    "cfg_long[\"grad_clip\"] = 1.0\n",
    "\n",
    "# (Optional) often helps for long runs: a slightly smaller LR\n",
    "cfg_long[\"lr\"] = cfg_long[\"lr\"] * 0.7\n",
    "\n",
    "OUT_LONG = \"/pscratch/sd/b/by1997/pkpd/training_runs/long/pk\"\n",
    "os.makedirs(OUT_LONG, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pk_times_t = torch.tensor(TG.PK_TIMES, dtype=torch.float32, device=device)\n",
    "\n",
    "best_long, best_ckpt_path = train_one_config(\n",
    "    run_name=\"PK_best_long\",\n",
    "    dataset=ds_pk,\n",
    "    idx_tr=idx_tr,\n",
    "    idx_va=idx_va,\n",
    "    num_classes=10,\n",
    "    reg_dim=TG.PK_PARAM_DIM,\n",
    "    device=device,\n",
    "    out_dir=OUT_LONG,\n",
    "    cfg=cfg_long,\n",
    "    times_t=pk_times_t,\n",
    "    resume_training=False,     # <-- from scratch\n",
    "    resume_ckpt_path=None\n",
    ")\n",
    "\n",
    "print(\"Best val metrics:\", best_long)\n",
    "print(\"Best checkpoint:\", best_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3697fa-7c99-4713-aea2-4fc67b9bc398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run when we want to resume training\n",
    "best, ckpt = train_one_config(\n",
    "    run_name=\"PK_best_long\",\n",
    "    dataset=ds_pk,\n",
    "    idx_tr=idx_tr,\n",
    "    idx_va=idx_va,\n",
    "    num_classes=10,\n",
    "    reg_dim=TG.PK_PARAM_DIM,\n",
    "    device=device,\n",
    "    out_dir=\"/pscratch/sd/b/by1997/pkpd/training_runs/long/pk\",\n",
    "    cfg=cfg_long,\n",
    "    times_t=pk_times_t,\n",
    "    resume_training=True,\n",
    "    resume_ckpt_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dfc6c2-6c7a-4abb-9e2a-6450317e696f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2e6d92-3eac-45b9-990b-9d5a7badf322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "dl_te = DataLoader(\n",
    "    Subset(ds_pk, idx_te),\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f673f02-b6d4-4cfb-8e4e-5dfdb54a9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ckpt_path = best_balanced[\"ckpt\"]   # or best_by_acc[\"ckpt\"], etc.\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "cfg = ckpt[\"cfg\"]\n",
    "model = CP.MultiTaskTransformer(\n",
    "    input_dim=2,\n",
    "    num_classes=10,\n",
    "    reg_dim=TG.PK_PARAM_DIM,\n",
    "    d_model=cfg[\"d_model\"],\n",
    "    nhead=cfg[\"nhead\"],\n",
    "    num_layers=cfg[\"num_layers\"],\n",
    "    dim_feedforward=cfg[\"dim_feedforward\"],\n",
    "    dropout=cfg[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "pk_times_t = torch.tensor(TG.PK_TIMES, dtype=torch.float32, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f142eb3-34ab-4893-8630-662208122769",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer_on_loader(model, loader, device, times_t, y_norm_obj):\n",
    "    all_pred_cls = []\n",
    "    all_true_cls = []\n",
    "    all_pred_params = []\n",
    "    all_true_params = []\n",
    "    all_masks = []\n",
    "\n",
    "    for x, y_cls, y_reg_norm, y_mask in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y_cls = y_cls.to(device, non_blocking=True)\n",
    "        y_reg_norm = y_reg_norm.to(device, non_blocking=True)\n",
    "        y_mask = y_mask.to(device, non_blocking=True)\n",
    "\n",
    "        logits, yhat_reg_norm = model(x, times_t)\n",
    "\n",
    "        pred_cls = logits.argmax(dim=1)\n",
    "\n",
    "        # move to CPU numpy\n",
    "        pred_cls_np = pred_cls.cpu().numpy()\n",
    "        true_cls_np = y_cls.cpu().numpy()\n",
    "        mask_np = y_mask.cpu().numpy()\n",
    "\n",
    "        # invert normalized reg targets + predictions into physical parameter values\n",
    "        yhat_np = yhat_reg_norm.cpu().numpy()\n",
    "        ytrue_np = y_reg_norm.cpu().numpy()\n",
    "\n",
    "        pred_params = np.stack([y_norm_obj.inverse_one(yhat_np[i], mask_np[i]) for i in range(len(yhat_np))])\n",
    "        true_params = np.stack([y_norm_obj.inverse_one(ytrue_np[i], mask_np[i]) for i in range(len(ytrue_np))])\n",
    "\n",
    "        all_pred_cls.append(pred_cls_np)\n",
    "        all_true_cls.append(true_cls_np)\n",
    "        all_pred_params.append(pred_params)\n",
    "        all_true_params.append(true_params)\n",
    "        all_masks.append(mask_np)\n",
    "\n",
    "    return {\n",
    "        \"pred_cls\": np.concatenate(all_pred_cls),\n",
    "        \"true_cls\": np.concatenate(all_true_cls),\n",
    "        \"pred_params\": np.concatenate(all_pred_params, axis=0),\n",
    "        \"true_params\": np.concatenate(all_true_params, axis=0),\n",
    "        \"mask\": np.concatenate(all_masks, axis=0),\n",
    "    }\n",
    "\n",
    "pk_test_out = infer_on_loader(model, dl_te, device, pk_times_t, ynorm_pk)\n",
    "print(\"pred_cls shape:\", pk_test_out[\"pred_cls\"].shape)\n",
    "print(\"pred_params shape:\", pk_test_out[\"pred_params\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e911ddc-f4c1-43d5-b73c-cd287e3217e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(pk_test_out[\"true_cls\"], pk_test_out[\"pred_cls\"])\n",
    "print(\"Test accuracy:\", acc)\n",
    "\n",
    "# masked MSE in physical space\n",
    "diff2 = (pk_test_out[\"pred_params\"] - pk_test_out[\"true_params\"])**2\n",
    "m = pk_test_out[\"mask\"]\n",
    "mse = (diff2 * m).sum() / np.maximum(m.sum(), 1.0)\n",
    "print(\"Test masked param MSE (physical):\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58ba99-7f6f-4f73-b31c-1aa170aeff5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My_Environement_2",
   "language": "python",
   "name": "rl_agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
